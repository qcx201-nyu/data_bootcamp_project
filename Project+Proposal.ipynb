{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Proposal & Data Report\n",
    "\n",
    "Jack (Quan Cheng) Xie | qcx201 | N14077607 <br>\n",
    "Professor Michael Waugh <br>\n",
    "Data Bootcamp <br>\n",
    "27 November 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of the project is to explore the English language. Languages abound with interesting and unintuitive attributes.\n",
    "\n",
    "##### The Zipf!\n",
    "One example of such attributes is the [**Zipf Mystery**](https://www.youtube.com/watch?v=fCn8zs912OE) *(video-link for the printout version: https://www.youtube.com/watch?v=fCn8zs912OE, or search  **The Zipf Mystery** by Vsauce on youtube)*. The Zipf mystery is a phenomenon which shows that the frequency of word-use follows a Pareto distribution (or also the power law) such that (roughly) 20% of words in the English lexicon take up 80% of usage. Why? No one knows.\n",
    "\n",
    "##### Questions, Questions...\n",
    "The starting point of the project will be to reproduce this *Zipf* phenomenon (and obviously, plot the curve). We can test out how the *Zipf* distribution might change (or not change) in different linguistic environments ( Reddit vs Twitter vs the New York Times *(and maybe vs works of literature)* ) and across time. Then, we may look into some of the possible reasons for this phenomenon, such as the random-words hypothesis mentioned in the video. That may lead us to explore what kinds of words fall into a particular part of the distribution.\n",
    "\n",
    "We can also see if the *Zipf* phenomenon appears in any other facets of language other than word-frequency in the basic form. Perhaps the *Zipf* may show up in not only single words but in phrases or word-groups. What about *Zipf* in semantically-related words? *Zipf* in sentiment? *Zipf* in the frequency of syllables? *Zipf* in rhymes? \n",
    "\n",
    "Maybe? Let's find out.\n",
    "\n",
    "\n",
    "##### The Data\n",
    "\n",
    "The datasets that we will use include a collection of comments from reddit, snippets from New York Times articles, and tweets. The python code for accessing or scraping the data are below. Other possibilities include samples from literature through the NLTK module, though I may have to get a little more familiar with NLTK first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import requests, io\n",
    "import zipfile as zf  \n",
    "import bz2\n",
    "import shutil\n",
    "import os\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import nltk as nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame Summary Function\n",
    "A function to summarize shape, columns, index, and return a DataFrame head-preview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def df_summary(dataframe):\n",
    "    print('Shape:', dataframe.shape, '\\n')\n",
    "    print('Columns:', list(dataframe.columns),'\\n')\n",
    "    print('Index:', dataframe.index)\n",
    "    return dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data I. Reddit Archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing and opening bz2 files\n",
    "def get_reddit_file(file):\n",
    "    url = 'https://files.pushshift.io/reddit/comments/'\n",
    "    print('Fetching:', file)\n",
    "    resp = requests.get(url+file)\n",
    "    print(file,resp)\n",
    "    result = bz2.BZ2File(io.BytesIO(resp.content))\n",
    "    clear_output()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining bz2 files into one json\n",
    "def get_reddit_archives(start):\n",
    "    years = list(range(2006,start+1))\n",
    "    for y in range(len(years)):\n",
    "        years[y] = str(years[y])\n",
    "\n",
    "    file_list = list()\n",
    "    initial = 'RC_2006-01.bz2'\n",
    "    for year in years:\n",
    "        file_list.append(initial.replace('2006',year))\n",
    "\n",
    "    result = list()\n",
    "    for file in file_list:\n",
    "        bz_file = get_reddit_file(file)\n",
    "        linelist = bz_file.readlines()\n",
    "        for line in linelist:\n",
    "            result.append(json.loads(line))\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execution and Pandas\n",
    "    # I've only done up to 2008 just to get an idea, otherwise it'll take a little too long\n",
    "    # I'll keepy trying to use the API to scrape Reddit comments\n",
    "    # But for now I will use the archives in case the API doesn't work out\n",
    "all_archives = get_reddit_archives(2008)\n",
    "reddit = pd.DataFrame(all_archives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (537997, 22) \n",
      "\n",
      "Columns: ['archived', 'author', 'author_flair_css_class', 'author_flair_text', 'body', 'controversiality', 'created_utc', 'distinguished', 'downs', 'edited', 'gilded', 'id', 'link_id', 'name', 'parent_id', 'retrieved_on', 'score', 'score_hidden', 'stickied', 'subreddit', 'subreddit_id', 'ups'] \n",
      "\n",
      "Index: RangeIndex(start=0, stop=537997, step=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archived</th>\n",
       "      <th>author</th>\n",
       "      <th>author_flair_css_class</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>body</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>downs</th>\n",
       "      <th>edited</th>\n",
       "      <th>...</th>\n",
       "      <th>link_id</th>\n",
       "      <th>name</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>retrieved_on</th>\n",
       "      <th>score</th>\n",
       "      <th>score_hidden</th>\n",
       "      <th>stickied</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>ups</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>jh99</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>early 2006 a probable date</td>\n",
       "      <td>0</td>\n",
       "      <td>1136074029</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_22569</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_22569</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>jpb</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>If you are going to post something that has a ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1136076410</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_22542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_22542</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Pichu0102</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Microsoft hates it's own products?\\r\\nWho knew?</td>\n",
       "      <td>0</td>\n",
       "      <td>1136078623</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_22515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_22515</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>libertas</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>this looks interesting, but it's already aired...</td>\n",
       "      <td>0</td>\n",
       "      <td>1136079346</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_22528</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_22528</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>mdmurray</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>I have nothing but good things to say about De...</td>\n",
       "      <td>0</td>\n",
       "      <td>1136081389</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>t3_22538</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t3_22538</td>\n",
       "      <td>1473821517</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t5_6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  archived     author author_flair_css_class author_flair_text  \\\n",
       "0      NaN       jh99                   None              None   \n",
       "1      NaN        jpb                   None              None   \n",
       "2      NaN  Pichu0102                   None              None   \n",
       "3      NaN   libertas                   None              None   \n",
       "4      NaN   mdmurray                   None              None   \n",
       "\n",
       "                                                body  controversiality  \\\n",
       "0                         early 2006 a probable date                 0   \n",
       "1  If you are going to post something that has a ...                 0   \n",
       "2    Microsoft hates it's own products?\\r\\nWho knew?                 0   \n",
       "3  this looks interesting, but it's already aired...                 0   \n",
       "4  I have nothing but good things to say about De...                 0   \n",
       "\n",
       "  created_utc distinguished  downs edited ...   link_id name parent_id  \\\n",
       "0  1136074029          None    NaN  False ...  t3_22569  NaN  t3_22569   \n",
       "1  1136076410          None    NaN  False ...  t3_22542  NaN  t3_22542   \n",
       "2  1136078623          None    NaN  False ...  t3_22515  NaN  t3_22515   \n",
       "3  1136079346          None    NaN  False ...  t3_22528  NaN  t3_22528   \n",
       "4  1136081389          None    NaN  False ...  t3_22538  NaN  t3_22538   \n",
       "\n",
       "  retrieved_on score  score_hidden  stickied   subreddit subreddit_id ups  \n",
       "0   1473821517     0           NaN     False  reddit.com         t5_6   0  \n",
       "1   1473821517     0           NaN     False  reddit.com         t5_6   0  \n",
       "2   1473821517     2           NaN     False  reddit.com         t5_6   2  \n",
       "3   1473821517     2           NaN     False  reddit.com         t5_6   2  \n",
       "4   1473821517     0           NaN     False  reddit.com         t5_6   0  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize DataFrame\n",
    "df_summary(reddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of Interest:\n",
    "`body` for the text data. `created_utc` for the time series analysis. The timeframe here is nice if I can get all the data from 2006 to 2017, but that may be a little bit difficult with the archive volume. The more recent years have too many instances to download and work with efficiently. For now I will use the archive data until I can figure out how to scrape comments from the Reddit API, which is a little complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                           early 2006 a probable date\n",
       "1    If you are going to post something that has a ...\n",
       "2      Microsoft hates it's own products?\\r\\nWho knew?\n",
       "3    this looks interesting, but it's already aired...\n",
       "4    I have nothing but good things to say about De...\n",
       "Name: body, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.head().body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1136074029\n",
       "1    1136076410\n",
       "2    1136078623\n",
       "3    1136079346\n",
       "4    1136081389\n",
       "Name: created_utc, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reddit.head().created_utc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data II. New York Times Articles API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retreiving data 'documents' from API call\n",
    "def nyt_data(year, month, day):\n",
    "    \n",
    "    url = \"https://api.nytimes.com/svc/search/v2/articlesearch.json\"\n",
    "    #nyt_key = 'e3fba4ee4f0944619aa6e7be1fc73eab' # Sometimes backups are needed because of the call limit\n",
    "    nyt_key = '770978643efb463e84871368163388d7'\n",
    "    #nyt_key = '7b36694d6ed447e4a3da0b2bf411d0f8'\n",
    "    \n",
    "    parameters = {'api-key' : nyt_key,\n",
    "              'begin_date' : year+month+day,\n",
    "              'sort' : 'oldest'}\n",
    "\n",
    "    resp = requests.get(url, params = parameters)\n",
    "    data = resp.json()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting articles\n",
    "def get_articles(year,month,day):\n",
    "    articles = list()\n",
    "    articles = nyt_data(year, month, day)['response']['docs']\n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Years and Months for iteration\n",
    "years = list(range(2006,2017+1))\n",
    "for y in range(len(years)):\n",
    "    years[y] = str(years[y])\n",
    "\n",
    "months = list()\n",
    "for m in range(1,12+1):\n",
    "    if m < 10:\n",
    "        months.append('0'+str(m))\n",
    "    else:\n",
    "        months.append(str(m))\n",
    "\n",
    "# We won't use this now, but it may come in handy later if we decide to up the scraping frequency.\n",
    "def days(month):\n",
    "    m30 = ['04','06','09','11']\n",
    "    res = list()\n",
    "    for d in range(1,10):\n",
    "        res.append('0'+str(d))   \n",
    "    if month == '02':\n",
    "        for d in range(10,28+1):\n",
    "            res.append(str(d))\n",
    "    elif month in m30:\n",
    "        for d in range(10,30+1):\n",
    "            res.append(str(d))\n",
    "    else:\n",
    "        for d in range(10,31+1):\n",
    "            res.append(str(d))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping data through iteration\n",
    "def get_nyt():\n",
    "    result = list()\n",
    "    for year in years:\n",
    "        print('Start:', year)\n",
    "        for month in months:\n",
    "            for day in ['01','15']: # Two datasets of 10 articles a month\n",
    "                try: # NYT API limits call rate, so a timer is set to slow down the call iteration\n",
    "                    docs = get_articles(year, month, day)\n",
    "                except:\n",
    "                    time.sleep(5)\n",
    "                    docs = get_articles(year, month, day)\n",
    "                    continue\n",
    "                for article in docs:\n",
    "                    result.append(article)\n",
    "                print(year, month, day)\n",
    "                time.sleep(2)\n",
    "        print(year, 'OK')\n",
    "        clear_output()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2820"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Executing... Wish me luck!\n",
    "nyt_api = get_nyt()\n",
    "len(nyt_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas DataFrame\n",
    "nyt = pd.DataFrame(nyt_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (2820, 20) \n",
      "\n",
      "Columns: ['_id', 'abstract', 'blog', 'byline', 'document_type', 'headline', 'keywords', 'multimedia', 'new_desk', 'print_page', 'pub_date', 'score', 'section_name', 'slideshow_credits', 'snippet', 'source', 'type_of_material', 'uri', 'web_url', 'word_count'] \n",
      "\n",
      "Index: RangeIndex(start=0, stop=2820, step=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>blog</th>\n",
       "      <th>byline</th>\n",
       "      <th>document_type</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>new_desk</th>\n",
       "      <th>print_page</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>score</th>\n",
       "      <th>section_name</th>\n",
       "      <th>slideshow_credits</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>type_of_material</th>\n",
       "      <th>uri</th>\n",
       "      <th>web_url</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4fd24e778eb7c8105d7f036c</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'original': 'By ALISON BERKLEY'}</td>\n",
       "      <td>article</td>\n",
       "      <td>{'main': 'Long, Steep and Lovely in Aspen', 'k...</td>\n",
       "      <td>[{'isMajor': None, 'rank': 0, 'name': 'glocati...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Travel Desk</td>\n",
       "      <td>4</td>\n",
       "      <td>2006-01-01T00:00:00Z</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>While jet setters schuss down the groomed slop...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.nytimes.com/2006/01/01/travel/01su...</td>\n",
       "      <td>568.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4fd24e778eb7c8105d7f0372</td>\n",
       "      <td>George Ernsberger letter on Jesse Green's arti...</td>\n",
       "      <td>{}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>article</td>\n",
       "      <td>{'main': 'The Making of an Ice Princess'}</td>\n",
       "      <td>[{'isMajor': None, 'rank': 0, 'name': 'persons...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Magazine</td>\n",
       "      <td>10</td>\n",
       "      <td>2006-01-01T00:00:00Z</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Thanks for Jesse Green's terrific article (Dec...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Letter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
       "      <td>41.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4fd24e778eb7c8105d7f037b</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'original': 'By KELLY FEENEY'}</td>\n",
       "      <td>article</td>\n",
       "      <td>{'main': 'Quick Bite/Millburn; When in Essex, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>New Jersey Weekly Desk</td>\n",
       "      <td>10</td>\n",
       "      <td>2006-01-01T00:00:00Z</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The word famiglia in a food store's name can b...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Review</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4fd24e778eb7c8105d7f0381</td>\n",
       "      <td>British and Dutch researchers conduct study on...</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'original': 'By ALEX WILLIAMS'}</td>\n",
       "      <td>article</td>\n",
       "      <td>{'main': 'Hangover Helpers: Beyond Sheep Eyes'}</td>\n",
       "      <td>[{'isMajor': None, 'rank': 0, 'name': 'glocati...</td>\n",
       "      <td>[{'type': 'image', 'subtype': 'thumbnail', 'ur...</td>\n",
       "      <td>Style Desk</td>\n",
       "      <td>1</td>\n",
       "      <td>2006-01-01T00:00:00Z</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>As people wake up from another New Year's Eve,...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.nytimes.com/2006/01/01/fashion/sun...</td>\n",
       "      <td>1346.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4fd24e778eb7c8105d7f038a</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'original': 'By HOWARD MARKEL'}</td>\n",
       "      <td>article</td>\n",
       "      <td>{'main': 'If the Avian Flu Hasn't Hit, Here's ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Week in Review Desk</td>\n",
       "      <td>10</td>\n",
       "      <td>2006-01-01T00:00:00Z</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WILD birds have completed their seasonal migra...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
       "      <td>329.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        _id  \\\n",
       "0  4fd24e778eb7c8105d7f036c   \n",
       "1  4fd24e778eb7c8105d7f0372   \n",
       "2  4fd24e778eb7c8105d7f037b   \n",
       "3  4fd24e778eb7c8105d7f0381   \n",
       "4  4fd24e778eb7c8105d7f038a   \n",
       "\n",
       "                                            abstract blog  \\\n",
       "0                                                NaN   {}   \n",
       "1  George Ernsberger letter on Jesse Green's arti...   {}   \n",
       "2                                                NaN   {}   \n",
       "3  British and Dutch researchers conduct study on...   {}   \n",
       "4                                                NaN   {}   \n",
       "\n",
       "                              byline document_type  \\\n",
       "0  {'original': 'By ALISON BERKLEY'}       article   \n",
       "1                                NaN       article   \n",
       "2    {'original': 'By KELLY FEENEY'}       article   \n",
       "3   {'original': 'By ALEX WILLIAMS'}       article   \n",
       "4   {'original': 'By HOWARD MARKEL'}       article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  {'main': 'Long, Steep and Lovely in Aspen', 'k...   \n",
       "1          {'main': 'The Making of an Ice Princess'}   \n",
       "2  {'main': 'Quick Bite/Millburn; When in Essex, ...   \n",
       "3    {'main': 'Hangover Helpers: Beyond Sheep Eyes'}   \n",
       "4  {'main': 'If the Avian Flu Hasn't Hit, Here's ...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  [{'isMajor': None, 'rank': 0, 'name': 'glocati...   \n",
       "1  [{'isMajor': None, 'rank': 0, 'name': 'persons...   \n",
       "2                                                 []   \n",
       "3  [{'isMajor': None, 'rank': 0, 'name': 'glocati...   \n",
       "4                                                 []   \n",
       "\n",
       "                                          multimedia                new_desk  \\\n",
       "0                                                 []             Travel Desk   \n",
       "1                                                 []                Magazine   \n",
       "2                                                 []  New Jersey Weekly Desk   \n",
       "3  [{'type': 'image', 'subtype': 'thumbnail', 'ur...              Style Desk   \n",
       "4                                                 []     Week in Review Desk   \n",
       "\n",
       "  print_page              pub_date  score section_name slideshow_credits  \\\n",
       "0          4  2006-01-01T00:00:00Z    1.0          NaN               NaN   \n",
       "1         10  2006-01-01T00:00:00Z    1.0          NaN               NaN   \n",
       "2         10  2006-01-01T00:00:00Z    1.0          NaN               NaN   \n",
       "3          1  2006-01-01T00:00:00Z    1.0          NaN               NaN   \n",
       "4         10  2006-01-01T00:00:00Z    1.0          NaN               NaN   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  While jet setters schuss down the groomed slop...  The New York Times   \n",
       "1  Thanks for Jesse Green's terrific article (Dec...  The New York Times   \n",
       "2  The word famiglia in a food store's name can b...  The New York Times   \n",
       "3  As people wake up from another New Year's Eve,...  The New York Times   \n",
       "4  WILD birds have completed their seasonal migra...  The New York Times   \n",
       "\n",
       "  type_of_material  uri                                            web_url  \\\n",
       "0             News  NaN  https://www.nytimes.com/2006/01/01/travel/01su...   \n",
       "1           Letter  NaN  https://query.nytimes.com/gst/fullpage.html?re...   \n",
       "2           Review  NaN  https://query.nytimes.com/gst/fullpage.html?re...   \n",
       "3             News  NaN  https://www.nytimes.com/2006/01/01/fashion/sun...   \n",
       "4             News  NaN  https://query.nytimes.com/gst/fullpage.html?re...   \n",
       "\n",
       "   word_count  \n",
       "0       568.0  \n",
       "1        41.0  \n",
       "2       350.0  \n",
       "3      1346.0  \n",
       "4       329.0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize DataFrame\n",
    "df_summary(nyt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of Interest:\n",
    "`pub_date` for time series analysis. `abstract` and `snippet` for text data, though these texts are often quite short compared to the full articles. Later I may try to increase the data size by making the date frequency finer, though it might be a little tricky with the API limitations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  NaN\n",
       "1    George Ernsberger letter on Jesse Green's arti...\n",
       "2                                                  NaN\n",
       "3    British and Dutch researchers conduct study on...\n",
       "4                                                  NaN\n",
       "Name: abstract, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt.head().abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    While jet setters schuss down the groomed slop...\n",
       "1    Thanks for Jesse Green's terrific article (Dec...\n",
       "2    The word famiglia in a food store's name can b...\n",
       "3    As people wake up from another New Year's Eve,...\n",
       "4    WILD birds have completed their seasonal migra...\n",
       "Name: snippet, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt.head().snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2006-01-01T00:00:00Z\n",
       "1    2006-01-01T00:00:00Z\n",
       "2    2006-01-01T00:00:00Z\n",
       "3    2006-01-01T00:00:00Z\n",
       "4    2006-01-01T00:00:00Z\n",
       "Name: pub_date, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nyt.head().pub_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data III. Twitter Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heatmap_x_usa_x_filter_nativeretweets.xlsx',\n",
       " '__MACOSX/',\n",
       " '__MACOSX/._heatmap_x_usa_x_filter_nativeretweets.xlsx',\n",
       " 'coverage_x_usa_x_filter_nativeretweets.xlsx',\n",
       " '__MACOSX/._coverage_x_usa_x_filter_nativeretweets.xlsx',\n",
       " 'geolocation_x_usa_x_filter_nativeretweets.xlsx',\n",
       " '__MACOSX/._geolocation_x_usa_x_filter_nativeretweets.xlsx',\n",
       " 'dashboard_x_usa_x_filter_nativeretweets.xlsx',\n",
       " '__MACOSX/._dashboard_x_usa_x_filter_nativeretweets.xlsx']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing the zip file\n",
    "url = 'http://followthehashtag.com/content/uploads/USA-Geolocated-tweets-free-dataset-Followthehashtag.zip'\n",
    "resp = requests.get(url)\n",
    "tw_zip = zf.ZipFile(io.BytesIO(resp.content))\n",
    "tw_zip.namelist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading DataFrame through Pandas\n",
    "twitter = pd.read_excel(tw_zip.open(tw_zip.namelist()[7]),sheet_name = 'Stream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Changing column names because of spaces\n",
    "twitter_cols = list(twitter.columns)\n",
    "new_twitter_cols = [twitter_cols[i].replace(' ','_') for i in range(len(twitter_cols))]\n",
    "twitter.columns = new_twitter_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (204820, 19) \n",
      "\n",
      "Columns: ['Tweet_Id', 'Date', 'Hour', 'User_Name', 'Nickname', 'Bio', 'Tweet_content', 'Favs', 'RTs', 'Latitude', 'Longitude', 'Country', 'Place_(as_appears_on_Bio)', 'Profile_picture', 'Followers', 'Following', 'Listed', 'Tweet_language_(ISO_639-1)', 'Tweet_Url'] \n",
      "\n",
      "Index: RangeIndex(start=0, stop=204820, step=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Hour</th>\n",
       "      <th>User_Name</th>\n",
       "      <th>Nickname</th>\n",
       "      <th>Bio</th>\n",
       "      <th>Tweet_content</th>\n",
       "      <th>Favs</th>\n",
       "      <th>RTs</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Country</th>\n",
       "      <th>Place_(as_appears_on_Bio)</th>\n",
       "      <th>Profile_picture</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Following</th>\n",
       "      <th>Listed</th>\n",
       "      <th>Tweet_language_(ISO_639-1)</th>\n",
       "      <th>Tweet_Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>721318437075685382</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>12:44</td>\n",
       "      <td>Bill Schulhoff</td>\n",
       "      <td>BillSchulhoff</td>\n",
       "      <td>Husband,Dad,GrandDad,Ordained Minister, Umpire...</td>\n",
       "      <td>Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.760278</td>\n",
       "      <td>-72.954722</td>\n",
       "      <td>US</td>\n",
       "      <td>East Patchogue, NY</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/3788000007...</td>\n",
       "      <td>386.0</td>\n",
       "      <td>705.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.twitter.com/BillSchulhoff/status/72...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>721318436173979648</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>12:44</td>\n",
       "      <td>Daniele Polis</td>\n",
       "      <td>danipolis</td>\n",
       "      <td>Viagens, geek, moda, batons laranja, cabelos c...</td>\n",
       "      <td>Pausa pro caf茅 antes de embarcar no pr贸ximo v么...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32.898349</td>\n",
       "      <td>-97.039196</td>\n",
       "      <td>US</td>\n",
       "      <td>Grapevine, TX</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7041760340...</td>\n",
       "      <td>812.0</td>\n",
       "      <td>647.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>pt</td>\n",
       "      <td>http://www.twitter.com/danipolis/status/721318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>721318434169102336</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>12:44</td>\n",
       "      <td>Kasey Jacobs</td>\n",
       "      <td>KJacobs27</td>\n",
       "      <td>Norwich University Class of 2017</td>\n",
       "      <td>Good. Morning. #morning #Saturday #diner #VT #...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.199476</td>\n",
       "      <td>-72.504173</td>\n",
       "      <td>US</td>\n",
       "      <td>Barre, VT</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/7169585649...</td>\n",
       "      <td>179.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.twitter.com/KJacobs27/status/721318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>721318429844582400</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>12:44</td>\n",
       "      <td>Stan Curtis</td>\n",
       "      <td>stncurtis</td>\n",
       "      <td>transcendental music, art for art's sake, craf...</td>\n",
       "      <td>@gratefuldead recordstoredayus 桂桂 @ TOMS MUSI...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.901474</td>\n",
       "      <td>-76.606817</td>\n",
       "      <td>US</td>\n",
       "      <td>Red Lion, PA</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/6962528246...</td>\n",
       "      <td>1229.0</td>\n",
       "      <td>2071.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.twitter.com/stncurtis/status/721318...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>721318429081407488</td>\n",
       "      <td>2016-04-16</td>\n",
       "      <td>12:44</td>\n",
       "      <td>Dave Borzymowski</td>\n",
       "      <td>wi_borzo</td>\n",
       "      <td>When in doubt....Panic.</td>\n",
       "      <td>Egg in a muffin!!! (@ Rocket Baby Bakery - @ro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43.060849</td>\n",
       "      <td>-87.998309</td>\n",
       "      <td>US</td>\n",
       "      <td>Wauwatosa, WI</td>\n",
       "      <td>http://pbs.twimg.com/profile_images/6595279129...</td>\n",
       "      <td>129.0</td>\n",
       "      <td>833.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>en</td>\n",
       "      <td>http://www.twitter.com/wi_borzo/status/7213184...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Tweet_Id        Date   Hour         User_Name       Nickname  \\\n",
       "0  721318437075685382  2016-04-16  12:44    Bill Schulhoff  BillSchulhoff   \n",
       "1  721318436173979648  2016-04-16  12:44     Daniele Polis      danipolis   \n",
       "2  721318434169102336  2016-04-16  12:44      Kasey Jacobs      KJacobs27   \n",
       "3  721318429844582400  2016-04-16  12:44       Stan Curtis      stncurtis   \n",
       "4  721318429081407488  2016-04-16  12:44  Dave Borzymowski       wi_borzo   \n",
       "\n",
       "                                                 Bio  \\\n",
       "0  Husband,Dad,GrandDad,Ordained Minister, Umpire...   \n",
       "1  Viagens, geek, moda, batons laranja, cabelos c...   \n",
       "2                   Norwich University Class of 2017   \n",
       "3  transcendental music, art for art's sake, craf...   \n",
       "4                            When in doubt....Panic.   \n",
       "\n",
       "                                       Tweet_content  Favs  RTs   Latitude  \\\n",
       "0  Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...   NaN  NaN  40.760278   \n",
       "1  Pausa pro caf茅 antes de embarcar no pr贸ximo v么...   NaN  NaN  32.898349   \n",
       "2  Good. Morning. #morning #Saturday #diner #VT #...   NaN  NaN  44.199476   \n",
       "3  @gratefuldead recordstoredayus 桂桂 @ TOMS MUSI...   NaN  NaN  39.901474   \n",
       "4  Egg in a muffin!!! (@ Rocket Baby Bakery - @ro...   NaN  NaN  43.060849   \n",
       "\n",
       "   Longitude Country Place_(as_appears_on_Bio)  \\\n",
       "0 -72.954722      US        East Patchogue, NY   \n",
       "1 -97.039196      US             Grapevine, TX   \n",
       "2 -72.504173      US                 Barre, VT   \n",
       "3 -76.606817      US              Red Lion, PA   \n",
       "4 -87.998309      US             Wauwatosa, WI   \n",
       "\n",
       "                                     Profile_picture  Followers  Following  \\\n",
       "0  http://pbs.twimg.com/profile_images/3788000007...      386.0      705.0   \n",
       "1  http://pbs.twimg.com/profile_images/7041760340...      812.0      647.0   \n",
       "2  http://pbs.twimg.com/profile_images/7169585649...      179.0      206.0   \n",
       "3  http://pbs.twimg.com/profile_images/6962528246...     1229.0     2071.0   \n",
       "4  http://pbs.twimg.com/profile_images/6595279129...      129.0      833.0   \n",
       "\n",
       "   Listed Tweet_language_(ISO_639-1)  \\\n",
       "0    24.0                         en   \n",
       "1    16.0                         pt   \n",
       "2     2.0                         en   \n",
       "3    11.0                         en   \n",
       "4     9.0                         en   \n",
       "\n",
       "                                           Tweet_Url  \n",
       "0  http://www.twitter.com/BillSchulhoff/status/72...  \n",
       "1  http://www.twitter.com/danipolis/status/721318...  \n",
       "2  http://www.twitter.com/KJacobs27/status/721318...  \n",
       "3  http://www.twitter.com/stncurtis/status/721318...  \n",
       "4  http://www.twitter.com/wi_borzo/status/7213184...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize DataFrame\n",
    "df_summary(twitter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Columns of Interest:\n",
    "`Tweet_content` for text data.\n",
    "`Date` for the time series analysis, though for this particular dataset the timeframe is very small because of the issues with large tweet volumes and Twitter's limitations on accessing older tweets through their API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Wind 3.2 mph NNE. Barometer 30.20 in, Rising s...\n",
       "1    Pausa pro caf茅 antes de embarcar no pr贸ximo v么...\n",
       "2    Good. Morning. #morning #Saturday #diner #VT #...\n",
       "3    @gratefuldead recordstoredayus 桂桂 @ TOMS MUSI...\n",
       "4    Egg in a muffin!!! (@ Rocket Baby Bakery - @ro...\n",
       "Name: Tweet_content, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head().Tweet_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2016-04-16\n",
       "1    2016-04-16\n",
       "2    2016-04-16\n",
       "3    2016-04-16\n",
       "4    2016-04-16\n",
       "Name: Date, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter.head().Date"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
